{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd8f7b22-7d19-442a-b647-adf2cb9f0b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mauro/projects/llm-python/cuda/lib/python3.12/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, n_layers, dropout=0.1):\n",
    "        super(SimpleTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model, nhead=n_heads, num_encoder_layers=n_layers, \n",
    "            num_decoder_layers=n_layers, dropout=dropout\n",
    "        )\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        src_emb = self.embedding(src).permute(1, 0, 2)\n",
    "        tgt_emb = self.embedding(tgt).permute(1, 0, 2)\n",
    "        output = self.transformer(src_emb, tgt_emb)\n",
    "        return self.fc_out(output)\n",
    "\n",
    "# Preparar os dados\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Definir hiperparâmetros\n",
    "vocab_size = tokenizer.vocab_size\n",
    "d_model = 512  # Dimensão dos embeddings\n",
    "n_heads = 8  # Número de cabeças de atenção\n",
    "n_layers = 6  # Número de camadas do Transformer\n",
    "\n",
    "# Criar o modelo\n",
    "model = SimpleTransformer(vocab_size, d_model, n_heads, n_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cba4c4b-892e-4f12-8850-84210a0767c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "175767ca-bca0-4745-ba71-c515c07d00c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de trechos: 1648\n",
      "Exemplo de trecho: Dom Casmurro\n"
     ]
    }
   ],
   "source": [
    "def load_text_file(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    return text\n",
    "\n",
    "# Carregar e dividir o texto em parágrafos\n",
    "file_path = \"dom_casmurro.txt\"\n",
    "full_text = load_text_file(file_path)\n",
    "texts = full_text.split(\"\\n\\n\")  # Divide em parágrafos\n",
    "texts = [t.strip() for t in texts if t.strip()]  # Remove espaços extras\n",
    "\n",
    "print(f\"Total de trechos: {len(texts)}\")\n",
    "print(f\"Exemplo de trecho: {texts[0][:200]}\")  # Exibir um trecho do texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c548dca1-f55f-4dc9-a26d-d0e473b32f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import time\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        # Pré-tokenizar todos os textos\n",
    "        self.tokens = [\n",
    "            tokenizer(\n",
    "                text, \n",
    "                max_length=max_length, \n",
    "                padding=\"max_length\", \n",
    "                truncation=True, \n",
    "                return_tensors=\"pt\"\n",
    "            )[\"input_ids\"].squeeze(0) \n",
    "            for text in texts\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.tokens[idx]\n",
    "        return input_ids, input_ids\n",
    "\n",
    "# Criar dataset com texto corrigido\n",
    "dataset = TextDataset(texts, tokenizer)\n",
    "\n",
    "# Definir função de collation para lidar com tamanhos diferentes\n",
    "def collate_fn(batch):\n",
    "    inputs, targets = zip(*batch)\n",
    "    inputs = torch.stack(inputs)  # Empilhar os tensores para batch\n",
    "    targets = torch.stack(targets)\n",
    "    return inputs, targets\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn, num_workers=0)\n",
    "\n",
    "# Loop de treinamento\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    for i, (src, tgt) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt)\n",
    "        loss = criterion(output.view(-1, vocab_size), tgt.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 10 == 0:  # Print a cada 10 batches\n",
    "            print(f\"Batch {i}, Time: {time.time() - start_time:.2f}s\")\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}, Time: {time.time() - start_time:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d11a209-b1ae-47f6-aa7c-2e04730666ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, model, tokenizer, max_length=50):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        tokens = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"]\n",
    "        for _ in range(max_length):\n",
    "            output = model(tokens, tokens)\n",
    "            next_token = output.argmax(-1)[:, -1].unsqueeze(0)\n",
    "            tokens = torch.cat([tokens, next_token], dim=1)\n",
    "    \n",
    "    return tokenizer.decode(tokens.squeeze(), skip_special_tokens=True)\n",
    "\n",
    "# Gerar um texto\n",
    "generated_text = generate_text(\"Olá, como você está?\", model, tokenizer)\n",
    "print(generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-gpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
